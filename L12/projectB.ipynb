{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1HrBoc7vWj-oQGdmNm4_kJwuWKTV7zINW",
      "authorship_tag": "ABX9TyOXrfxWxJ+0cyqZ6FWaYo46",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heyaug/homework/blob/master/projectB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uO5vfMvLgF95",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import log_loss\n",
        "import xgboost as xgb\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbgFZJqxgItR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRN='/content/drive/My Drive/train_ver2.csv'\n",
        "TST='/content/drive/My Drive/test_ver2.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sE_RHfKlgQEo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_cols=['ind_ahor_fin_ult1', 'ind_aval_fin_ult1', 'ind_cco_fin_ult1', 'ind_cder_fin_ult1',\n",
        "                   'ind_cno_fin_ult1', 'ind_ctju_fin_ult1', 'ind_ctma_fin_ult1', 'ind_ctop_fin_ult1',\n",
        "                   'ind_ctpp_fin_ult1', 'ind_deco_fin_ult1', 'ind_deme_fin_ult1', 'ind_dela_fin_ult1',\n",
        "                   'ind_ecue_fin_ult1', 'ind_fond_fin_ult1', 'ind_hip_fin_ult1', 'ind_plan_fin_ult1',\n",
        "                   'ind_pres_fin_ult1', 'ind_reca_fin_ult1', 'ind_tjcr_fin_ult1', 'ind_valo_fin_ult1',\n",
        "                   'ind_viv_fin_ult1', 'ind_nomina_ult1', 'ind_nom_pens_ult1', 'ind_recibo_ult1']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xp6TSoVgUi3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use 2015-06 data only\n",
        "trn=pd.read_csv(TRN, usecols=target_cols + ['fecha_dato', 'ncodpers'])\n",
        "trn_may=trn[trn['fecha_dato']=='2015-05-28'].drop(['fecha_dato'],axis=1).set_index('ncodpers')\n",
        "trn_jun=trn[trn['fecha_dato']=='2015-06-28'].drop(['fecha_dato'],axis=1).set_index('ncodpers')\n",
        "trn_may.columns=[col+'_may' for col in target_cols]\n",
        "trn_jun.columns=[col+'_jun' for col in target_cols]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INN-2iOVgr7B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "b622a40d-a4cf-43ad-f4b4-b945bf066c15"
      },
      "source": [
        "labels=trn_jun.join(trn_may)\n",
        "labels.fillna(0,inplace=True)\n",
        "for col in target_cols:\n",
        "  labels[col]=labels[col+'_jun']-labels[col+'_may']\n",
        "labels=labels[target_cols]\n",
        "index=labels[labels.sum(axis=1)>0].index\n",
        "labels=labels.loc[index]\n",
        "\n",
        "trn=pd.read_csv(TRN)\n",
        "trn=trn[trn['fecha_dato']=='2015-06-28'].set_index('ncodpers')\n",
        "trn=trn.loc[index]\n",
        "trn[target_cols]=labels\n",
        "\n",
        "del labels,index,trn_may,trn_jun"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (5,8,11,15) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H48Q9f2ghWXh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# melt 24 classes into single multi-class\n",
        "trn_melt=[]\n",
        "for ind, (run, row) in enumerate(trn.iterrows()):\n",
        "    for i in range(24):\n",
        "        if row[23 + i] == 1:\n",
        "            temp = row[:23].values.tolist()\n",
        "            temp.append(i)\n",
        "            trn_melt.append(temp)\n",
        "trn = pd.DataFrame(np.asarray(trn_melt), columns=trn.columns.tolist()[:23] + ['target'])\n",
        "y=trn['target'].astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCXfMCFnhlLg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "428581e8-e5e1-450a-d268-78eaed8087f6"
      },
      "source": [
        "rem_targets=[23, 22, 2, 21, 18, 17, 4, 11, 12, 9, 6, 13, 7, 19, 8]\n",
        "trn=trn[y.isin(rem_targets)]\n",
        "y=y[y.isin(rem_targets)]\n",
        "y=LabelEncoder().fit_transform(y)\n",
        "\n",
        "tst=pd.read_csv(TST)\n",
        "tst.drop(['ncodpers'], axis=1, inplace=True)\n",
        "print('#trn shape : {} | yshape : {} | tst shape : {}'.format(trn.shape, y.shape, tst.shape))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (15) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "# trn shape : (38230, 24) | y shape : (38230,) | tst shape : (929615, 23)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zsuqux2_hv_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cols_to_use=['pais_residencia','sexo','age','antiguedad','canal_entrada','cod_prov','renta','segmento']\n",
        "trn=trn[cols_to_use]\n",
        "tst=tst[cols_to_use]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNPo4GCUh7Ng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "categoricals=['pais_residencia','sexo','canal_entrada','segmento']\n",
        "for col in categoricals:\n",
        "    temp,_=pd.concat([trn[col],tst[col]],axis=0).factorize()\n",
        "    trn[col]=temp[:trn.shape[0]]\n",
        "    tst[col]=temp[trn.shape[0]:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtPVELu7iIj6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn.fillna(-99,inplace=True)\n",
        "tst.fillna(-99,inplace=True)\n",
        "\n",
        "trn['age'].replace(' NA',-99,inplace=True)\n",
        "trn['age']=trn['age'].astype(int)\n",
        "trn['antiguedad'].replace('     NA', -99, inplace=True)\n",
        "trn['antiguedad']=trn['antiguedad'].astype(int)\n",
        "trn['cod_prov']=trn['cod_prov'].replace('nan',-99).astype(float).astype(int)\n",
        "tst['cod_prov']=tst['cod_prov'].replace('nan',-99).astype(float).astype(int)\n",
        "trn['renta']=trn['renta'].replace('nan', -99).astype(float).astype(int)\n",
        "tst['renta']=tst['renta'].replace('         NA',-99).astype(float).astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8plAl9zGiUnR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn.replace(-99,np.nan,inplace=True)\n",
        "tst.replace(-99,np.nan,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nH9Rbjs0ilUe",
        "colab_type": "text"
      },
      "source": [
        "XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GtIE8gnijce",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cfb1e394-f611-42f4-bc39-defbf31f0c60"
      },
      "source": [
        "num_round=500\n",
        "early_stop=10\n",
        "xgb_params={\n",
        "    'booster':'gbtree',\n",
        "    'max_depth': 2,\n",
        "    'nthread': 4,\n",
        "    'num_class': 15,\n",
        "    'objective': 'multi:softprob',\n",
        "    'silent': 1,\n",
        "    'eval_metric': 'mlogloss',\n",
        "    'seed': 777,\n",
        "}\n",
        "\n",
        "trn_scores = []\n",
        "vld_scores = []\n",
        "best_iters = []\n",
        "n_splits = 5\n",
        "sss = StratifiedShuffleSplit(n_splits=n_splits, test_size=0.1, random_state=777)\n",
        "for i, (t_ind, v_ind) in enumerate(sss.split(trn, y)):\n",
        "    print('# Iter {} / {}'.format(i+1, n_splits))\n",
        "    x_trn = np.asarray(trn)[t_ind]\n",
        "    x_vld = np.asarray(trn)[v_ind]\n",
        "    y_trn = np.asarray(y)[t_ind]\n",
        "    y_vld = np.asarray(y)[v_ind]\n",
        "\n",
        "    dtrn = xgb.DMatrix(x_trn, label=y_trn)\n",
        "    dvld = xgb.DMatrix(x_vld, label=y_vld)\n",
        "    watch_list = [(dtrn, 'train'), (dvld, 'eval')]\n",
        "\n",
        "    # fit xgb\n",
        "    bst = xgb.train(xgb_params, dtrn, num_round, watch_list,\n",
        "                    early_stopping_rounds=early_stop, verbose_eval=True)\n",
        "\n",
        "    # eval _ trn\n",
        "    score = log_loss(y_trn, bst.predict(dtrn))\n",
        "    trn_scores.append(score)\n",
        "\n",
        "    # eval _ vld\n",
        "    score = log_loss(y_vld, bst.predict(dvld))\n",
        "    vld_scores.append(score)\n",
        "\n",
        "    # best iters\n",
        "    best_iters.append(bst.best_iteration)\n",
        "\n",
        "print('# TRN logloss: {}'.format(np.mean(trn_scores)))\n",
        "print('# VLD logloss: {}'.format(np.mean(vld_scores)))\n",
        "print('# Best Iters : {}'.format(np.mean(best_iters)))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# Iter 1 / 5\n",
            "[0]\ttrain-mlogloss:2.39421\teval-mlogloss:2.39689\n",
            "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
            "\n",
            "Will train until eval-mlogloss hasn't improved in 10 rounds.\n",
            "[1]\ttrain-mlogloss:2.25339\teval-mlogloss:2.25702\n",
            "[2]\ttrain-mlogloss:2.15941\teval-mlogloss:2.16425\n",
            "[3]\ttrain-mlogloss:2.09137\teval-mlogloss:2.09666\n",
            "[4]\ttrain-mlogloss:2.04084\teval-mlogloss:2.0464\n",
            "[5]\ttrain-mlogloss:2.00206\teval-mlogloss:2.00856\n",
            "[6]\ttrain-mlogloss:1.9718\teval-mlogloss:1.97868\n",
            "[7]\ttrain-mlogloss:1.94778\teval-mlogloss:1.95507\n",
            "[8]\ttrain-mlogloss:1.92837\teval-mlogloss:1.93671\n",
            "[9]\ttrain-mlogloss:1.91261\teval-mlogloss:1.92168\n",
            "[10]\ttrain-mlogloss:1.90002\teval-mlogloss:1.91003\n",
            "[11]\ttrain-mlogloss:1.88957\teval-mlogloss:1.89994\n",
            "[12]\ttrain-mlogloss:1.88096\teval-mlogloss:1.89239\n",
            "[13]\ttrain-mlogloss:1.8737\teval-mlogloss:1.88603\n",
            "[14]\ttrain-mlogloss:1.86754\teval-mlogloss:1.88057\n",
            "[15]\ttrain-mlogloss:1.86238\teval-mlogloss:1.87634\n",
            "[16]\ttrain-mlogloss:1.85805\teval-mlogloss:1.87315\n",
            "[17]\ttrain-mlogloss:1.8542\teval-mlogloss:1.8704\n",
            "[18]\ttrain-mlogloss:1.851\teval-mlogloss:1.86811\n",
            "[19]\ttrain-mlogloss:1.8482\teval-mlogloss:1.86607\n",
            "[20]\ttrain-mlogloss:1.84576\teval-mlogloss:1.86411\n",
            "[21]\ttrain-mlogloss:1.84368\teval-mlogloss:1.86289\n",
            "[22]\ttrain-mlogloss:1.84181\teval-mlogloss:1.86184\n",
            "[23]\ttrain-mlogloss:1.8401\teval-mlogloss:1.8606\n",
            "[24]\ttrain-mlogloss:1.83819\teval-mlogloss:1.85959\n",
            "[25]\ttrain-mlogloss:1.83677\teval-mlogloss:1.85907\n",
            "[26]\ttrain-mlogloss:1.83522\teval-mlogloss:1.8581\n",
            "[27]\ttrain-mlogloss:1.83387\teval-mlogloss:1.85793\n",
            "[28]\ttrain-mlogloss:1.83272\teval-mlogloss:1.85718\n",
            "[29]\ttrain-mlogloss:1.83161\teval-mlogloss:1.85694\n",
            "[30]\ttrain-mlogloss:1.8307\teval-mlogloss:1.85671\n",
            "[31]\ttrain-mlogloss:1.82975\teval-mlogloss:1.85634\n",
            "[32]\ttrain-mlogloss:1.82878\teval-mlogloss:1.85638\n",
            "[33]\ttrain-mlogloss:1.82793\teval-mlogloss:1.85607\n",
            "[34]\ttrain-mlogloss:1.82713\teval-mlogloss:1.85554\n",
            "[35]\ttrain-mlogloss:1.82613\teval-mlogloss:1.85528\n",
            "[36]\ttrain-mlogloss:1.82544\teval-mlogloss:1.85505\n",
            "[37]\ttrain-mlogloss:1.82472\teval-mlogloss:1.85488\n",
            "[38]\ttrain-mlogloss:1.82396\teval-mlogloss:1.85495\n",
            "[39]\ttrain-mlogloss:1.82324\teval-mlogloss:1.85497\n",
            "[40]\ttrain-mlogloss:1.82268\teval-mlogloss:1.85487\n",
            "[41]\ttrain-mlogloss:1.82208\teval-mlogloss:1.85457\n",
            "[42]\ttrain-mlogloss:1.82132\teval-mlogloss:1.85483\n",
            "[43]\ttrain-mlogloss:1.82056\teval-mlogloss:1.85428\n",
            "[44]\ttrain-mlogloss:1.81998\teval-mlogloss:1.8541\n",
            "[45]\ttrain-mlogloss:1.81929\teval-mlogloss:1.85397\n",
            "[46]\ttrain-mlogloss:1.81856\teval-mlogloss:1.85396\n",
            "[47]\ttrain-mlogloss:1.81787\teval-mlogloss:1.85403\n",
            "[48]\ttrain-mlogloss:1.81728\teval-mlogloss:1.85378\n",
            "[49]\ttrain-mlogloss:1.8166\teval-mlogloss:1.85384\n",
            "[50]\ttrain-mlogloss:1.81603\teval-mlogloss:1.85381\n",
            "[51]\ttrain-mlogloss:1.81548\teval-mlogloss:1.8539\n",
            "[52]\ttrain-mlogloss:1.81473\teval-mlogloss:1.85382\n",
            "[53]\ttrain-mlogloss:1.81393\teval-mlogloss:1.8538\n",
            "[54]\ttrain-mlogloss:1.81334\teval-mlogloss:1.85374\n",
            "[55]\ttrain-mlogloss:1.81285\teval-mlogloss:1.85352\n",
            "[56]\ttrain-mlogloss:1.81229\teval-mlogloss:1.85355\n",
            "[57]\ttrain-mlogloss:1.81168\teval-mlogloss:1.8535\n",
            "[58]\ttrain-mlogloss:1.81097\teval-mlogloss:1.85318\n",
            "[59]\ttrain-mlogloss:1.81059\teval-mlogloss:1.85326\n",
            "[60]\ttrain-mlogloss:1.8101\teval-mlogloss:1.85361\n",
            "[61]\ttrain-mlogloss:1.80959\teval-mlogloss:1.85334\n",
            "[62]\ttrain-mlogloss:1.80907\teval-mlogloss:1.85339\n",
            "[63]\ttrain-mlogloss:1.80862\teval-mlogloss:1.85362\n",
            "[64]\ttrain-mlogloss:1.80809\teval-mlogloss:1.85378\n",
            "[65]\ttrain-mlogloss:1.80753\teval-mlogloss:1.85368\n",
            "[66]\ttrain-mlogloss:1.80705\teval-mlogloss:1.85401\n",
            "[67]\ttrain-mlogloss:1.80666\teval-mlogloss:1.85422\n",
            "[68]\ttrain-mlogloss:1.806\teval-mlogloss:1.85397\n",
            "Stopping. Best iteration:\n",
            "[58]\ttrain-mlogloss:1.81097\teval-mlogloss:1.85318\n",
            "\n",
            "# Iter 2 / 5\n",
            "[0]\ttrain-mlogloss:2.39425\teval-mlogloss:2.39069\n",
            "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
            "\n",
            "Will train until eval-mlogloss hasn't improved in 10 rounds.\n",
            "[1]\ttrain-mlogloss:2.2539\teval-mlogloss:2.24962\n",
            "[2]\ttrain-mlogloss:2.16008\teval-mlogloss:2.15572\n",
            "[3]\ttrain-mlogloss:2.09261\teval-mlogloss:2.08786\n",
            "[4]\ttrain-mlogloss:2.04196\teval-mlogloss:2.03725\n",
            "[5]\ttrain-mlogloss:2.00284\teval-mlogloss:1.99811\n",
            "[6]\ttrain-mlogloss:1.9727\teval-mlogloss:1.9681\n",
            "[7]\ttrain-mlogloss:1.94897\teval-mlogloss:1.94451\n",
            "[8]\ttrain-mlogloss:1.92964\teval-mlogloss:1.92583\n",
            "[9]\ttrain-mlogloss:1.91397\teval-mlogloss:1.91096\n",
            "[10]\ttrain-mlogloss:1.90135\teval-mlogloss:1.89869\n",
            "[11]\ttrain-mlogloss:1.89095\teval-mlogloss:1.88894\n",
            "[12]\ttrain-mlogloss:1.88247\teval-mlogloss:1.88097\n",
            "[13]\ttrain-mlogloss:1.87515\teval-mlogloss:1.87417\n",
            "[14]\ttrain-mlogloss:1.86922\teval-mlogloss:1.86898\n",
            "[15]\ttrain-mlogloss:1.86408\teval-mlogloss:1.86451\n",
            "[16]\ttrain-mlogloss:1.85975\teval-mlogloss:1.86088\n",
            "[17]\ttrain-mlogloss:1.85591\teval-mlogloss:1.85752\n",
            "[18]\ttrain-mlogloss:1.8527\teval-mlogloss:1.85507\n",
            "[19]\ttrain-mlogloss:1.84991\teval-mlogloss:1.85277\n",
            "[20]\ttrain-mlogloss:1.84747\teval-mlogloss:1.85112\n",
            "[21]\ttrain-mlogloss:1.84507\teval-mlogloss:1.84937\n",
            "[22]\ttrain-mlogloss:1.84312\teval-mlogloss:1.84833\n",
            "[23]\ttrain-mlogloss:1.84143\teval-mlogloss:1.84696\n",
            "[24]\ttrain-mlogloss:1.83984\teval-mlogloss:1.84597\n",
            "[25]\ttrain-mlogloss:1.83817\teval-mlogloss:1.84492\n",
            "[26]\ttrain-mlogloss:1.83673\teval-mlogloss:1.84418\n",
            "[27]\ttrain-mlogloss:1.83552\teval-mlogloss:1.84323\n",
            "[28]\ttrain-mlogloss:1.83439\teval-mlogloss:1.84274\n",
            "[29]\ttrain-mlogloss:1.83326\teval-mlogloss:1.84192\n",
            "[30]\ttrain-mlogloss:1.83229\teval-mlogloss:1.84129\n",
            "[31]\ttrain-mlogloss:1.83139\teval-mlogloss:1.84076\n",
            "[32]\ttrain-mlogloss:1.83051\teval-mlogloss:1.84018\n",
            "[33]\ttrain-mlogloss:1.82964\teval-mlogloss:1.83988\n",
            "[34]\ttrain-mlogloss:1.82886\teval-mlogloss:1.8398\n",
            "[35]\ttrain-mlogloss:1.828\teval-mlogloss:1.83959\n",
            "[36]\ttrain-mlogloss:1.82719\teval-mlogloss:1.83926\n",
            "[37]\ttrain-mlogloss:1.82656\teval-mlogloss:1.83898\n",
            "[38]\ttrain-mlogloss:1.82577\teval-mlogloss:1.83855\n",
            "[39]\ttrain-mlogloss:1.82511\teval-mlogloss:1.83837\n",
            "[40]\ttrain-mlogloss:1.82442\teval-mlogloss:1.83814\n",
            "[41]\ttrain-mlogloss:1.82389\teval-mlogloss:1.83805\n",
            "[42]\ttrain-mlogloss:1.82316\teval-mlogloss:1.83777\n",
            "[43]\ttrain-mlogloss:1.82265\teval-mlogloss:1.83771\n",
            "[44]\ttrain-mlogloss:1.82203\teval-mlogloss:1.83771\n",
            "[45]\ttrain-mlogloss:1.82131\teval-mlogloss:1.83733\n",
            "[46]\ttrain-mlogloss:1.82074\teval-mlogloss:1.83718\n",
            "[47]\ttrain-mlogloss:1.8201\teval-mlogloss:1.83719\n",
            "[48]\ttrain-mlogloss:1.81953\teval-mlogloss:1.83724\n",
            "[49]\ttrain-mlogloss:1.81887\teval-mlogloss:1.8373\n",
            "[50]\ttrain-mlogloss:1.81827\teval-mlogloss:1.83727\n",
            "[51]\ttrain-mlogloss:1.81762\teval-mlogloss:1.83706\n",
            "[52]\ttrain-mlogloss:1.81701\teval-mlogloss:1.837\n",
            "[53]\ttrain-mlogloss:1.8165\teval-mlogloss:1.83683\n",
            "[54]\ttrain-mlogloss:1.8159\teval-mlogloss:1.83671\n",
            "[55]\ttrain-mlogloss:1.81539\teval-mlogloss:1.83682\n",
            "[56]\ttrain-mlogloss:1.81466\teval-mlogloss:1.8367\n",
            "[57]\ttrain-mlogloss:1.81392\teval-mlogloss:1.83623\n",
            "[58]\ttrain-mlogloss:1.81335\teval-mlogloss:1.83625\n",
            "[59]\ttrain-mlogloss:1.81293\teval-mlogloss:1.83633\n",
            "[60]\ttrain-mlogloss:1.81242\teval-mlogloss:1.83623\n",
            "[61]\ttrain-mlogloss:1.81191\teval-mlogloss:1.83602\n",
            "[62]\ttrain-mlogloss:1.8115\teval-mlogloss:1.83603\n",
            "[63]\ttrain-mlogloss:1.81105\teval-mlogloss:1.83619\n",
            "[64]\ttrain-mlogloss:1.81046\teval-mlogloss:1.83602\n",
            "[65]\ttrain-mlogloss:1.80993\teval-mlogloss:1.83573\n",
            "[66]\ttrain-mlogloss:1.8095\teval-mlogloss:1.8358\n",
            "[67]\ttrain-mlogloss:1.80904\teval-mlogloss:1.83546\n",
            "[68]\ttrain-mlogloss:1.8085\teval-mlogloss:1.8351\n",
            "[69]\ttrain-mlogloss:1.80797\teval-mlogloss:1.83519\n",
            "[70]\ttrain-mlogloss:1.80754\teval-mlogloss:1.83526\n",
            "[71]\ttrain-mlogloss:1.80707\teval-mlogloss:1.8353\n",
            "[72]\ttrain-mlogloss:1.80663\teval-mlogloss:1.83525\n",
            "[73]\ttrain-mlogloss:1.80622\teval-mlogloss:1.83521\n",
            "[74]\ttrain-mlogloss:1.80568\teval-mlogloss:1.83522\n",
            "[75]\ttrain-mlogloss:1.80523\teval-mlogloss:1.8354\n",
            "[76]\ttrain-mlogloss:1.8047\teval-mlogloss:1.83508\n",
            "[77]\ttrain-mlogloss:1.80429\teval-mlogloss:1.83508\n",
            "[78]\ttrain-mlogloss:1.80388\teval-mlogloss:1.83512\n",
            "[79]\ttrain-mlogloss:1.80347\teval-mlogloss:1.83518\n",
            "[80]\ttrain-mlogloss:1.803\teval-mlogloss:1.83541\n",
            "[81]\ttrain-mlogloss:1.80249\teval-mlogloss:1.83566\n",
            "[82]\ttrain-mlogloss:1.80212\teval-mlogloss:1.83592\n",
            "[83]\ttrain-mlogloss:1.80157\teval-mlogloss:1.83595\n",
            "[84]\ttrain-mlogloss:1.80112\teval-mlogloss:1.83607\n",
            "[85]\ttrain-mlogloss:1.80065\teval-mlogloss:1.83585\n",
            "[86]\ttrain-mlogloss:1.80036\teval-mlogloss:1.83579\n",
            "[87]\ttrain-mlogloss:1.79995\teval-mlogloss:1.83565\n",
            "Stopping. Best iteration:\n",
            "[77]\ttrain-mlogloss:1.80429\teval-mlogloss:1.83508\n",
            "\n",
            "# Iter 3 / 5\n",
            "[0]\ttrain-mlogloss:2.39258\teval-mlogloss:2.40087\n",
            "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
            "\n",
            "Will train until eval-mlogloss hasn't improved in 10 rounds.\n",
            "[1]\ttrain-mlogloss:2.25252\teval-mlogloss:2.2611\n",
            "[2]\ttrain-mlogloss:2.15884\teval-mlogloss:2.16796\n",
            "[3]\ttrain-mlogloss:2.09111\teval-mlogloss:2.1001\n",
            "[4]\ttrain-mlogloss:2.0405\teval-mlogloss:2.04976\n",
            "[5]\ttrain-mlogloss:2.00164\teval-mlogloss:2.01098\n",
            "[6]\ttrain-mlogloss:1.97166\teval-mlogloss:1.9811\n",
            "[7]\ttrain-mlogloss:1.94788\teval-mlogloss:1.95754\n",
            "[8]\ttrain-mlogloss:1.92863\teval-mlogloss:1.93839\n",
            "[9]\ttrain-mlogloss:1.91311\teval-mlogloss:1.92338\n",
            "[10]\ttrain-mlogloss:1.9005\teval-mlogloss:1.91036\n",
            "[11]\ttrain-mlogloss:1.89007\teval-mlogloss:1.90007\n",
            "[12]\ttrain-mlogloss:1.88159\teval-mlogloss:1.8918\n",
            "[13]\ttrain-mlogloss:1.87442\teval-mlogloss:1.88517\n",
            "[14]\ttrain-mlogloss:1.8684\teval-mlogloss:1.87945\n",
            "[15]\ttrain-mlogloss:1.86329\teval-mlogloss:1.87424\n",
            "[16]\ttrain-mlogloss:1.85887\teval-mlogloss:1.86965\n",
            "[17]\ttrain-mlogloss:1.85524\teval-mlogloss:1.86612\n",
            "[18]\ttrain-mlogloss:1.8521\teval-mlogloss:1.86345\n",
            "[19]\ttrain-mlogloss:1.84933\teval-mlogloss:1.86103\n",
            "[20]\ttrain-mlogloss:1.84676\teval-mlogloss:1.85891\n",
            "[21]\ttrain-mlogloss:1.84479\teval-mlogloss:1.85706\n",
            "[22]\ttrain-mlogloss:1.84289\teval-mlogloss:1.85553\n",
            "[23]\ttrain-mlogloss:1.84124\teval-mlogloss:1.8542\n",
            "[24]\ttrain-mlogloss:1.83913\teval-mlogloss:1.85331\n",
            "[25]\ttrain-mlogloss:1.83763\teval-mlogloss:1.85265\n",
            "[26]\ttrain-mlogloss:1.83627\teval-mlogloss:1.85208\n",
            "[27]\ttrain-mlogloss:1.83495\teval-mlogloss:1.85149\n",
            "[28]\ttrain-mlogloss:1.8336\teval-mlogloss:1.85086\n",
            "[29]\ttrain-mlogloss:1.8324\teval-mlogloss:1.85041\n",
            "[30]\ttrain-mlogloss:1.8315\teval-mlogloss:1.8498\n",
            "[31]\ttrain-mlogloss:1.8305\teval-mlogloss:1.84944\n",
            "[32]\ttrain-mlogloss:1.82954\teval-mlogloss:1.84883\n",
            "[33]\ttrain-mlogloss:1.82862\teval-mlogloss:1.84849\n",
            "[34]\ttrain-mlogloss:1.82781\teval-mlogloss:1.84794\n",
            "[35]\ttrain-mlogloss:1.82706\teval-mlogloss:1.84739\n",
            "[36]\ttrain-mlogloss:1.82636\teval-mlogloss:1.84709\n",
            "[37]\ttrain-mlogloss:1.82563\teval-mlogloss:1.84683\n",
            "[38]\ttrain-mlogloss:1.82472\teval-mlogloss:1.84627\n",
            "[39]\ttrain-mlogloss:1.824\teval-mlogloss:1.84608\n",
            "[40]\ttrain-mlogloss:1.82348\teval-mlogloss:1.8461\n",
            "[41]\ttrain-mlogloss:1.82281\teval-mlogloss:1.84591\n",
            "[42]\ttrain-mlogloss:1.82219\teval-mlogloss:1.84561\n",
            "[43]\ttrain-mlogloss:1.82142\teval-mlogloss:1.84528\n",
            "[44]\ttrain-mlogloss:1.82073\teval-mlogloss:1.84523\n",
            "[45]\ttrain-mlogloss:1.82008\teval-mlogloss:1.84501\n",
            "[46]\ttrain-mlogloss:1.81945\teval-mlogloss:1.84484\n",
            "[47]\ttrain-mlogloss:1.81895\teval-mlogloss:1.84474\n",
            "[48]\ttrain-mlogloss:1.81835\teval-mlogloss:1.84451\n",
            "[49]\ttrain-mlogloss:1.81777\teval-mlogloss:1.84444\n",
            "[50]\ttrain-mlogloss:1.81711\teval-mlogloss:1.84431\n",
            "[51]\ttrain-mlogloss:1.81656\teval-mlogloss:1.8443\n",
            "[52]\ttrain-mlogloss:1.81599\teval-mlogloss:1.844\n",
            "[53]\ttrain-mlogloss:1.8155\teval-mlogloss:1.84406\n",
            "[54]\ttrain-mlogloss:1.81492\teval-mlogloss:1.8441\n",
            "[55]\ttrain-mlogloss:1.81425\teval-mlogloss:1.84374\n",
            "[56]\ttrain-mlogloss:1.81376\teval-mlogloss:1.84392\n",
            "[57]\ttrain-mlogloss:1.81314\teval-mlogloss:1.84415\n",
            "[58]\ttrain-mlogloss:1.81271\teval-mlogloss:1.84409\n",
            "[59]\ttrain-mlogloss:1.81202\teval-mlogloss:1.84385\n",
            "[60]\ttrain-mlogloss:1.81155\teval-mlogloss:1.84385\n",
            "[61]\ttrain-mlogloss:1.81093\teval-mlogloss:1.84361\n",
            "[62]\ttrain-mlogloss:1.81047\teval-mlogloss:1.8436\n",
            "[63]\ttrain-mlogloss:1.81001\teval-mlogloss:1.84348\n",
            "[64]\ttrain-mlogloss:1.80954\teval-mlogloss:1.84349\n",
            "[65]\ttrain-mlogloss:1.80887\teval-mlogloss:1.84354\n",
            "[66]\ttrain-mlogloss:1.80845\teval-mlogloss:1.84337\n",
            "[67]\ttrain-mlogloss:1.80791\teval-mlogloss:1.84348\n",
            "[68]\ttrain-mlogloss:1.80739\teval-mlogloss:1.84324\n",
            "[69]\ttrain-mlogloss:1.80693\teval-mlogloss:1.84317\n",
            "[70]\ttrain-mlogloss:1.80647\teval-mlogloss:1.84307\n",
            "[71]\ttrain-mlogloss:1.80599\teval-mlogloss:1.84334\n",
            "[72]\ttrain-mlogloss:1.80549\teval-mlogloss:1.84329\n",
            "[73]\ttrain-mlogloss:1.80507\teval-mlogloss:1.84336\n",
            "[74]\ttrain-mlogloss:1.80462\teval-mlogloss:1.84322\n",
            "[75]\ttrain-mlogloss:1.80408\teval-mlogloss:1.84325\n",
            "[76]\ttrain-mlogloss:1.8036\teval-mlogloss:1.84305\n",
            "[77]\ttrain-mlogloss:1.8033\teval-mlogloss:1.84289\n",
            "[78]\ttrain-mlogloss:1.80294\teval-mlogloss:1.84276\n",
            "[79]\ttrain-mlogloss:1.80239\teval-mlogloss:1.84277\n",
            "[80]\ttrain-mlogloss:1.80181\teval-mlogloss:1.84272\n",
            "[81]\ttrain-mlogloss:1.80144\teval-mlogloss:1.84275\n",
            "[82]\ttrain-mlogloss:1.80104\teval-mlogloss:1.84284\n",
            "[83]\ttrain-mlogloss:1.80061\teval-mlogloss:1.84287\n",
            "[84]\ttrain-mlogloss:1.80021\teval-mlogloss:1.84289\n",
            "[85]\ttrain-mlogloss:1.79982\teval-mlogloss:1.84308\n",
            "[86]\ttrain-mlogloss:1.79938\teval-mlogloss:1.84328\n",
            "[87]\ttrain-mlogloss:1.79901\teval-mlogloss:1.84349\n",
            "[88]\ttrain-mlogloss:1.79846\teval-mlogloss:1.84358\n",
            "[89]\ttrain-mlogloss:1.79808\teval-mlogloss:1.84369\n",
            "[90]\ttrain-mlogloss:1.79757\teval-mlogloss:1.84387\n",
            "Stopping. Best iteration:\n",
            "[80]\ttrain-mlogloss:1.80181\teval-mlogloss:1.84272\n",
            "\n",
            "# Iter 4 / 5\n",
            "[0]\ttrain-mlogloss:2.39555\teval-mlogloss:2.39148\n",
            "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
            "\n",
            "Will train until eval-mlogloss hasn't improved in 10 rounds.\n",
            "[1]\ttrain-mlogloss:2.25428\teval-mlogloss:2.24998\n",
            "[2]\ttrain-mlogloss:2.16016\teval-mlogloss:2.15592\n",
            "[3]\ttrain-mlogloss:2.0922\teval-mlogloss:2.08846\n",
            "[4]\ttrain-mlogloss:2.04153\teval-mlogloss:2.03858\n",
            "[5]\ttrain-mlogloss:2.00231\teval-mlogloss:1.99964\n",
            "[6]\ttrain-mlogloss:1.97206\teval-mlogloss:1.96989\n",
            "[7]\ttrain-mlogloss:1.9483\teval-mlogloss:1.9465\n",
            "[8]\ttrain-mlogloss:1.92919\teval-mlogloss:1.92813\n",
            "[9]\ttrain-mlogloss:1.9137\teval-mlogloss:1.91368\n",
            "[10]\ttrain-mlogloss:1.90095\teval-mlogloss:1.90127\n",
            "[11]\ttrain-mlogloss:1.89074\teval-mlogloss:1.89166\n",
            "[12]\ttrain-mlogloss:1.88221\teval-mlogloss:1.8839\n",
            "[13]\ttrain-mlogloss:1.87498\teval-mlogloss:1.87734\n",
            "[14]\ttrain-mlogloss:1.86901\teval-mlogloss:1.87213\n",
            "[15]\ttrain-mlogloss:1.86406\teval-mlogloss:1.86763\n",
            "[16]\ttrain-mlogloss:1.85963\teval-mlogloss:1.86372\n",
            "[17]\ttrain-mlogloss:1.85595\teval-mlogloss:1.86063\n",
            "[18]\ttrain-mlogloss:1.85279\teval-mlogloss:1.85829\n",
            "[19]\ttrain-mlogloss:1.84989\teval-mlogloss:1.85611\n",
            "[20]\ttrain-mlogloss:1.84745\teval-mlogloss:1.8544\n",
            "[21]\ttrain-mlogloss:1.84508\teval-mlogloss:1.8527\n",
            "[22]\ttrain-mlogloss:1.84308\teval-mlogloss:1.85138\n",
            "[23]\ttrain-mlogloss:1.84126\teval-mlogloss:1.85025\n",
            "[24]\ttrain-mlogloss:1.8394\teval-mlogloss:1.84928\n",
            "[25]\ttrain-mlogloss:1.83785\teval-mlogloss:1.8485\n",
            "[26]\ttrain-mlogloss:1.83667\teval-mlogloss:1.84803\n",
            "[27]\ttrain-mlogloss:1.83545\teval-mlogloss:1.84743\n",
            "[28]\ttrain-mlogloss:1.83411\teval-mlogloss:1.84699\n",
            "[29]\ttrain-mlogloss:1.83317\teval-mlogloss:1.8465\n",
            "[30]\ttrain-mlogloss:1.83205\teval-mlogloss:1.84589\n",
            "[31]\ttrain-mlogloss:1.83111\teval-mlogloss:1.84549\n",
            "[32]\ttrain-mlogloss:1.83018\teval-mlogloss:1.84533\n",
            "[33]\ttrain-mlogloss:1.82926\teval-mlogloss:1.84474\n",
            "[34]\ttrain-mlogloss:1.82842\teval-mlogloss:1.84455\n",
            "[35]\ttrain-mlogloss:1.82766\teval-mlogloss:1.84431\n",
            "[36]\ttrain-mlogloss:1.82697\teval-mlogloss:1.84394\n",
            "[37]\ttrain-mlogloss:1.82621\teval-mlogloss:1.8435\n",
            "[38]\ttrain-mlogloss:1.82543\teval-mlogloss:1.8434\n",
            "[39]\ttrain-mlogloss:1.82477\teval-mlogloss:1.84327\n",
            "[40]\ttrain-mlogloss:1.82394\teval-mlogloss:1.84329\n",
            "[41]\ttrain-mlogloss:1.82309\teval-mlogloss:1.84297\n",
            "[42]\ttrain-mlogloss:1.82249\teval-mlogloss:1.84276\n",
            "[43]\ttrain-mlogloss:1.8219\teval-mlogloss:1.84278\n",
            "[44]\ttrain-mlogloss:1.82121\teval-mlogloss:1.84259\n",
            "[45]\ttrain-mlogloss:1.82058\teval-mlogloss:1.84258\n",
            "[46]\ttrain-mlogloss:1.81973\teval-mlogloss:1.84256\n",
            "[47]\ttrain-mlogloss:1.81911\teval-mlogloss:1.84254\n",
            "[48]\ttrain-mlogloss:1.81859\teval-mlogloss:1.84265\n",
            "[49]\ttrain-mlogloss:1.81793\teval-mlogloss:1.84238\n",
            "[50]\ttrain-mlogloss:1.81736\teval-mlogloss:1.84222\n",
            "[51]\ttrain-mlogloss:1.81677\teval-mlogloss:1.84238\n",
            "[52]\ttrain-mlogloss:1.81605\teval-mlogloss:1.84234\n",
            "[53]\ttrain-mlogloss:1.81549\teval-mlogloss:1.8425\n",
            "[54]\ttrain-mlogloss:1.81498\teval-mlogloss:1.84236\n",
            "[55]\ttrain-mlogloss:1.81444\teval-mlogloss:1.84243\n",
            "[56]\ttrain-mlogloss:1.81401\teval-mlogloss:1.84249\n",
            "[57]\ttrain-mlogloss:1.81354\teval-mlogloss:1.84234\n",
            "[58]\ttrain-mlogloss:1.81301\teval-mlogloss:1.84245\n",
            "[59]\ttrain-mlogloss:1.81251\teval-mlogloss:1.84238\n",
            "[60]\ttrain-mlogloss:1.81188\teval-mlogloss:1.84245\n",
            "Stopping. Best iteration:\n",
            "[50]\ttrain-mlogloss:1.81736\teval-mlogloss:1.84222\n",
            "\n",
            "# Iter 5 / 5\n",
            "[0]\ttrain-mlogloss:2.39325\teval-mlogloss:2.39744\n",
            "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
            "\n",
            "Will train until eval-mlogloss hasn't improved in 10 rounds.\n",
            "[1]\ttrain-mlogloss:2.25297\teval-mlogloss:2.25764\n",
            "[2]\ttrain-mlogloss:2.15923\teval-mlogloss:2.16436\n",
            "[3]\ttrain-mlogloss:2.09167\teval-mlogloss:2.09675\n",
            "[4]\ttrain-mlogloss:2.04088\teval-mlogloss:2.04656\n",
            "[5]\ttrain-mlogloss:2.00207\teval-mlogloss:2.0076\n",
            "[6]\ttrain-mlogloss:1.97195\teval-mlogloss:1.97804\n",
            "[7]\ttrain-mlogloss:1.94803\teval-mlogloss:1.95471\n",
            "[8]\ttrain-mlogloss:1.92866\teval-mlogloss:1.93629\n",
            "[9]\ttrain-mlogloss:1.91323\teval-mlogloss:1.92174\n",
            "[10]\ttrain-mlogloss:1.90053\teval-mlogloss:1.90969\n",
            "[11]\ttrain-mlogloss:1.88979\teval-mlogloss:1.89954\n",
            "[12]\ttrain-mlogloss:1.88133\teval-mlogloss:1.89188\n",
            "[13]\ttrain-mlogloss:1.87408\teval-mlogloss:1.88546\n",
            "[14]\ttrain-mlogloss:1.86801\teval-mlogloss:1.88002\n",
            "[15]\ttrain-mlogloss:1.86293\teval-mlogloss:1.87577\n",
            "[16]\ttrain-mlogloss:1.85856\teval-mlogloss:1.87215\n",
            "[17]\ttrain-mlogloss:1.85482\teval-mlogloss:1.86919\n",
            "[18]\ttrain-mlogloss:1.85156\teval-mlogloss:1.86672\n",
            "[19]\ttrain-mlogloss:1.84868\teval-mlogloss:1.86453\n",
            "[20]\ttrain-mlogloss:1.84631\teval-mlogloss:1.86277\n",
            "[21]\ttrain-mlogloss:1.84415\teval-mlogloss:1.86129\n",
            "[22]\ttrain-mlogloss:1.84182\teval-mlogloss:1.85972\n",
            "[23]\ttrain-mlogloss:1.83999\teval-mlogloss:1.85847\n",
            "[24]\ttrain-mlogloss:1.83854\teval-mlogloss:1.85772\n",
            "[25]\ttrain-mlogloss:1.83694\teval-mlogloss:1.85681\n",
            "[26]\ttrain-mlogloss:1.83538\teval-mlogloss:1.85535\n",
            "[27]\ttrain-mlogloss:1.83425\teval-mlogloss:1.85494\n",
            "[28]\ttrain-mlogloss:1.83294\teval-mlogloss:1.85396\n",
            "[29]\ttrain-mlogloss:1.83189\teval-mlogloss:1.8535\n",
            "[30]\ttrain-mlogloss:1.83076\teval-mlogloss:1.85289\n",
            "[31]\ttrain-mlogloss:1.82984\teval-mlogloss:1.85253\n",
            "[32]\ttrain-mlogloss:1.82892\teval-mlogloss:1.85181\n",
            "[33]\ttrain-mlogloss:1.828\teval-mlogloss:1.85138\n",
            "[34]\ttrain-mlogloss:1.8273\teval-mlogloss:1.85117\n",
            "[35]\ttrain-mlogloss:1.82632\teval-mlogloss:1.85051\n",
            "[36]\ttrain-mlogloss:1.8256\teval-mlogloss:1.85023\n",
            "[37]\ttrain-mlogloss:1.82484\teval-mlogloss:1.85\n",
            "[38]\ttrain-mlogloss:1.82411\teval-mlogloss:1.85004\n",
            "[39]\ttrain-mlogloss:1.82343\teval-mlogloss:1.85024\n",
            "[40]\ttrain-mlogloss:1.82269\teval-mlogloss:1.85036\n",
            "[41]\ttrain-mlogloss:1.82201\teval-mlogloss:1.84989\n",
            "[42]\ttrain-mlogloss:1.82125\teval-mlogloss:1.84983\n",
            "[43]\ttrain-mlogloss:1.82061\teval-mlogloss:1.84987\n",
            "[44]\ttrain-mlogloss:1.82003\teval-mlogloss:1.84977\n",
            "[45]\ttrain-mlogloss:1.81945\teval-mlogloss:1.84965\n",
            "[46]\ttrain-mlogloss:1.81875\teval-mlogloss:1.84981\n",
            "[47]\ttrain-mlogloss:1.81795\teval-mlogloss:1.8498\n",
            "[48]\ttrain-mlogloss:1.81741\teval-mlogloss:1.84979\n",
            "[49]\ttrain-mlogloss:1.81683\teval-mlogloss:1.84987\n",
            "[50]\ttrain-mlogloss:1.81617\teval-mlogloss:1.8498\n",
            "[51]\ttrain-mlogloss:1.81553\teval-mlogloss:1.84965\n",
            "[52]\ttrain-mlogloss:1.81491\teval-mlogloss:1.84956\n",
            "[53]\ttrain-mlogloss:1.81431\teval-mlogloss:1.84939\n",
            "[54]\ttrain-mlogloss:1.8137\teval-mlogloss:1.8495\n",
            "[55]\ttrain-mlogloss:1.81332\teval-mlogloss:1.84973\n",
            "[56]\ttrain-mlogloss:1.81294\teval-mlogloss:1.84978\n",
            "[57]\ttrain-mlogloss:1.81241\teval-mlogloss:1.84964\n",
            "[58]\ttrain-mlogloss:1.81184\teval-mlogloss:1.84977\n",
            "[59]\ttrain-mlogloss:1.81125\teval-mlogloss:1.84952\n",
            "[60]\ttrain-mlogloss:1.81078\teval-mlogloss:1.84961\n",
            "[61]\ttrain-mlogloss:1.81013\teval-mlogloss:1.84943\n",
            "[62]\ttrain-mlogloss:1.80971\teval-mlogloss:1.84981\n",
            "[63]\ttrain-mlogloss:1.8092\teval-mlogloss:1.84984\n",
            "Stopping. Best iteration:\n",
            "[53]\ttrain-mlogloss:1.81431\teval-mlogloss:1.84939\n",
            "\n",
            "# TRN logloss: 1.80492205227727\n",
            "# VLD logloss: 1.8451546930579608\n",
            "# Best Iters : 63.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ip-YJK23kWre",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dtrn=xgb.DMatrix(trn, label=y)\n",
        "num_round=int(np.mean(best_iters)/0.9)\n",
        "bst=xgb.train(xgb_params,dtrn,num_round,verbose_eval=False)\n",
        "\n",
        "dtst=xgb.DMatrix(tst)\n",
        "preds=bst.predict(dtst)\n",
        "preds=np.fliplr(np.argsort(preds,axis=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILkjKoqjdiMr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submit_cols=[target_cols[i] for i, col in enumerate(target_cols) if i in rem_targets]\n",
        "\n",
        "final_preds=[]\n",
        "for pred in preds:\n",
        "  top_products = []\n",
        "  for i, product in enumerate(pred):\n",
        "    top_products.append(submit_cols[product])\n",
        "    if i == 6:\n",
        "      break\n",
        "  final_preds.append(' '.join(top_products))\n",
        "\n",
        "t_index=pd.read_csv(TST, usecols=['ncodpers'])\n",
        "test_id=t_index['ncodpers']\n",
        "out_df=pd.DataFrame({'ncodpers': test_id, 'added_products': final_preds})\n",
        "out_df.to_csv(\"/content/drive/My Drive/result.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvBg6SKUpDsY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}